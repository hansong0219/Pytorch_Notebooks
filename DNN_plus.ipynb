{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "concrete-british",
   "metadata": {},
   "source": [
    "## Data augmentation과 Dropout 의 DNN 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-generation",
   "metadata": {},
   "source": [
    "일반적으로, 과적합을 방지하기 위해서 Dropout 층을 이용한다. Drop out 의 경우, 일부에만 역전파를 적용하여, 학습에 절반정도의 신경망만 사용하는 방법이다. \n",
    "본문에서는 Pytorch 에서 Dropout 과 Data Augmentation 의 적용 코드를 다룬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "southeast-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chinese-scotland",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Torch vision 의 transforms 는 입력을 변환시키는 도구로, Tensor 변환, Resize(), Normalize(), 등의 편리한 기능을 많이 가지고 있다.\n",
    "#사용법은 다음과 같다.\n",
    "#Data Augmentation을 위하여,transforms.Compose() 함수에 transforms.RandomHorizontalFlip 을 이용하여 무작위 수평 뒤집기를 수행하였다.\n",
    "#Test Transform 은 변환하지 않는다.\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3081,))])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "macro-culture",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS=50\n",
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ethical-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.FashionMNIST(root = './data/', train = True, download = True, transform = train_transform)\n",
    "testset = datasets.FashionMNIST(root = './data/', train = False, download = True, transform = test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lyric-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(dataset=trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=testset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "available-lottery",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#GPU 사용 코드 CUDA \n",
    "USE_CUDA = torch.cuda.is_available() \n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "documented-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    #__init__ 변수에 Dropout 이 추가 되었다.\n",
    "    def __init__(self, dropout_p = 0.2):\n",
    "        super(Net, self).__init__()\n",
    "        # 층의 입력과 출력을 변수로 둔다.\n",
    "        self.fc1 = nn.Linear(784,256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,10)\n",
    "        \n",
    "        #Dropout 확률을 변수로 둔다.\n",
    "        self.dropout_p = dropout_p        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x)) #가중치가 없는 연산의 경우 torch.nn.functional 에 있는 함수를 사용하기도 한다.\n",
    "        # 드롭 아웃층 추가\n",
    "        x = F.dropout(x, training=self.training, p=self.dropout_p)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training, p=self.dropout_p)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "academic-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(dropout_p=0.2).to(DEVICE) #모델을 cuda 메모리로 보내 처리하기 위해선 to() 함수를 써야한다.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lucky-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train() #학습/ 평가 모드에 따라 동작이 다른 모듈이 있기에 학습 모드로 전환 (항상 사용해 버릇 하는 것이 좋음)\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #학습 데이터를 DEVICE 메모리로 전달한다.\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "        \n",
    "    #평가과정에서는 기울기를 계산할 필요가 없기에 torch.no_grad() 를 명시해주어야함\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "                \n",
    "            # 모든 오차 더하기\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "                \n",
    "            # 가장 큰 값을 가진 클래스가 모델의 예측임\n",
    "            # 예측과 정답을 비교하여 일치할경우 coreect에 1 을 더함\n",
    "            pred = output.max(1, keepdim=True)[1] #output.max() 함수는 가장 큰값(확률)과 그 값이 있는 인덱스를 추출한다.\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() #eq() 함수는 일치하면 1을 아니면 0 을 출력함\n",
    "        \n",
    "    #전체 데이터셋에 대한 오차와 맞힌 개수의 합\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100.* correct/len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "parliamentary-renaissance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Test Loss: 0.6438, Accuracy: 77.44%\n",
      "[2] Test Loss: 0.5354, Accuracy: 80.82%\n",
      "[3] Test Loss: 0.4945, Accuracy: 81.94%\n",
      "[4] Test Loss: 0.4614, Accuracy: 83.31%\n",
      "[5] Test Loss: 0.4494, Accuracy: 84.03%\n",
      "[6] Test Loss: 0.4322, Accuracy: 84.39%\n",
      "[7] Test Loss: 0.4134, Accuracy: 84.96%\n",
      "[8] Test Loss: 0.4053, Accuracy: 85.53%\n",
      "[9] Test Loss: 0.4002, Accuracy: 85.63%\n",
      "[10] Test Loss: 0.3907, Accuracy: 85.76%\n",
      "[11] Test Loss: 0.3848, Accuracy: 86.04%\n",
      "[12] Test Loss: 0.3788, Accuracy: 86.33%\n",
      "[13] Test Loss: 0.3741, Accuracy: 86.36%\n",
      "[14] Test Loss: 0.3674, Accuracy: 86.74%\n",
      "[15] Test Loss: 0.3676, Accuracy: 86.89%\n",
      "[16] Test Loss: 0.3608, Accuracy: 87.06%\n",
      "[17] Test Loss: 0.3619, Accuracy: 86.84%\n",
      "[18] Test Loss: 0.3523, Accuracy: 87.35%\n",
      "[19] Test Loss: 0.3565, Accuracy: 87.39%\n",
      "[20] Test Loss: 0.3487, Accuracy: 87.58%\n",
      "[21] Test Loss: 0.3512, Accuracy: 87.34%\n",
      "[22] Test Loss: 0.3424, Accuracy: 87.90%\n",
      "[23] Test Loss: 0.3540, Accuracy: 87.17%\n",
      "[24] Test Loss: 0.3512, Accuracy: 87.37%\n",
      "[25] Test Loss: 0.3368, Accuracy: 88.23%\n",
      "[26] Test Loss: 0.3353, Accuracy: 87.92%\n",
      "[27] Test Loss: 0.3339, Accuracy: 88.28%\n",
      "[28] Test Loss: 0.3333, Accuracy: 88.00%\n",
      "[29] Test Loss: 0.3292, Accuracy: 88.13%\n",
      "[30] Test Loss: 0.3283, Accuracy: 88.52%\n",
      "[31] Test Loss: 0.3275, Accuracy: 88.36%\n",
      "[32] Test Loss: 0.3350, Accuracy: 87.74%\n",
      "[33] Test Loss: 0.3249, Accuracy: 88.45%\n",
      "[34] Test Loss: 0.3224, Accuracy: 88.36%\n",
      "[35] Test Loss: 0.3224, Accuracy: 88.51%\n",
      "[36] Test Loss: 0.3213, Accuracy: 88.35%\n",
      "[37] Test Loss: 0.3186, Accuracy: 88.53%\n",
      "[38] Test Loss: 0.3147, Accuracy: 88.78%\n",
      "[39] Test Loss: 0.3169, Accuracy: 88.72%\n",
      "[40] Test Loss: 0.3142, Accuracy: 88.67%\n",
      "[41] Test Loss: 0.3153, Accuracy: 88.79%\n",
      "[42] Test Loss: 0.3185, Accuracy: 88.65%\n",
      "[43] Test Loss: 0.3127, Accuracy: 88.62%\n",
      "[44] Test Loss: 0.3228, Accuracy: 88.46%\n",
      "[45] Test Loss: 0.3113, Accuracy: 88.90%\n",
      "[46] Test Loss: 0.3193, Accuracy: 88.62%\n",
      "[47] Test Loss: 0.3129, Accuracy: 88.71%\n",
      "[48] Test Loss: 0.3229, Accuracy: 88.65%\n",
      "[49] Test Loss: 0.3052, Accuracy: 89.09%\n",
      "[50] Test Loss: 0.3084, Accuracy: 89.01%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    train(model, train_loader, optimizer)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-weapon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
